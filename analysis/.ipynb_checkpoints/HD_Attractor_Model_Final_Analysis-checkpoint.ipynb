{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086d70ab",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "from ripser import ripser as tda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d7104",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba908b79",
   "metadata": {},
   "source": [
    "Run this section if you want to generate the data again. You don't need to do this as the data is already in the data folder. If you do want to regenerate all of the data then you need to set up your own directories and paths to provide a destination for the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"YOUR PATH\"\n",
    "dir_path = r\"C:\\home\\...\\HD_attractor_model\\data\"\n",
    "file_name_list = [r\"\\Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle\",\n",
    "                  r\"\\Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle_1\",\n",
    "                  r\"\\Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle_2\",\n",
    "                  r\"\\Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle_3\",\n",
    "                  r\"\\Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle_4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd51e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_name_list:\n",
    "    mat_file = loadmat(dir_path + file_name + \".mat\")\n",
    "    X = np.sqrt(mat_file['rHD'].T)\n",
    "    embedding = Isomap(n_neighbors=200, n_components=10)\n",
    "    tmp_emb = embedding.fit_transform(X)\n",
    "    np.save(save_path+file_name+r'_tmp_emb.npy', tmp_emb) #np.load(save_path+r'\\tmp_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd78704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_suffix_from_stem(stem: str) -> str:\n",
    "    m = re.search(r\"_(\\d+)$\", stem)\n",
    "    return f\"_{m.group(1)}\" if m else \"\"\n",
    "\n",
    "\n",
    "def build_segments_to_exclude_list_notebook(mat_file) -> np.ndarray:\n",
    "\n",
    "    starts = mat_file[\"stat_startIndices\"][0]\n",
    "    ends   = mat_file[\"stat_endIndices\"][0]\n",
    "\n",
    "    segments_to_exclude_list = []\n",
    "    for start, end in zip(starts, ends):\n",
    "        start = int(start)\n",
    "        end   = int(end)\n",
    "        segments_to_exclude_list.extend(list(range(start, end)))  # end excluded (NOTEBOOK)\n",
    "\n",
    "    # de-duplicate while preserving order (NOTEBOOK)\n",
    "    unique_list = []\n",
    "    seen = set()\n",
    "    for num in segments_to_exclude_list:\n",
    "        if num not in seen:\n",
    "            unique_list.append(num)\n",
    "            seen.add(num)\n",
    "\n",
    "    return np.asarray(unique_list, dtype=int)\n",
    "\n",
    "\n",
    "def angular_differences_notebook(real_angles, decoded_angles):\n",
    "    differences = []\n",
    "    for real_angle, decoded_angle in zip(real_angles, decoded_angles):\n",
    "        diff = abs(real_angle - decoded_angle)\n",
    "        circular_diff = np.minimum(diff, 360 - diff)\n",
    "        differences.append(circular_diff)\n",
    "    return differences\n",
    "\n",
    "\n",
    "def decoding_error_notebook_exact(mat_file) -> dict:\n",
    "    Az = mat_file[\"Az\"]\n",
    "    real_Az = (Az + 180) % 360\n",
    "\n",
    "    segments_to_exclude_list = build_segments_to_exclude_list_notebook(mat_file)\n",
    "\n",
    "    loco_real_Az = np.delete(real_Az[0], segments_to_exclude_list, axis=0)\n",
    "    stat_real_Az = real_Az[0][segments_to_exclude_list]\n",
    "\n",
    "    loco_decodedHD = np.delete(mat_file[\"decodedHD\"], segments_to_exclude_list, axis=0) + 180\n",
    "    stat_decodedHD = mat_file[\"decodedHD\"][segments_to_exclude_list] + 180\n",
    "\n",
    "    loco_errors = angular_differences_notebook(loco_real_Az, loco_decodedHD)\n",
    "    stat_errors = angular_differences_notebook(stat_real_Az, stat_decodedHD)\n",
    "\n",
    "    loco_errors_list = [loco_errors[i][0] for i in range(len(loco_real_Az))]\n",
    "    stat_errors_list = [stat_errors[i][0] for i in range(len(stat_real_Az))]\n",
    "\n",
    "    mean_loco_errors = float(np.mean(loco_errors_list))\n",
    "    sem_loco_errors  = float(np.std(loco_errors_list) / np.sqrt(len(loco_errors_list)))\n",
    "\n",
    "    mean_stat_errors = float(np.mean(stat_errors_list)) if len(stat_errors_list) else np.nan\n",
    "    sem_stat_errors  = float(np.std(stat_errors_list) / np.sqrt(len(stat_errors_list))) if len(stat_errors_list) else np.nan\n",
    "\n",
    "    return dict(\n",
    "        mean_loco_errors=mean_loco_errors,\n",
    "        sem_loco_errors=sem_loco_errors,\n",
    "        mean_stat_errors=mean_stat_errors,\n",
    "        sem_stat_errors=sem_stat_errors,\n",
    "        segments_to_exclude_list=segments_to_exclude_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_betti(isomap_emb: np.ndarray):\n",
    "    X = np.asarray(isomap_emb)\n",
    "    barcodes_01 = tda(X, maxdim=1, coeff=2)[\"dgms\"]\n",
    "    h0 = barcodes_01[0]\n",
    "    h1 = barcodes_01[1]\n",
    "\n",
    "    if len(X) > 1500:\n",
    "        idx = np.random.choice(np.arange(len(X)), 1500, replace=False)\n",
    "        X2 = X[idx]\n",
    "    else:\n",
    "        X2 = X\n",
    "    barcodes_012 = tda(X2, maxdim=2, coeff=2)[\"dgms\"]\n",
    "    h2 = barcodes_012[2]\n",
    "    return h0, h1, h2\n",
    "\n",
    "\n",
    "def circular_diff_deg(a_deg, b_deg):\n",
    "    a = np.asarray(a_deg, dtype=float)\n",
    "    b = np.asarray(b_deg, dtype=float)\n",
    "    return ((a - b + 180.0) % 360.0) - 180.0\n",
    "\n",
    "\n",
    "def compute_pref_nonpref_mean_activity(rHD: np.ndarray, real_Az_deg_0_360: np.ndarray, azimuth_range_deg=30.0):\n",
    "    N, T = rHD.shape\n",
    "    HD_PD_deg = np.linspace(-180.0, 180.0, N, endpoint=False)\n",
    "\n",
    "    pref_mean = np.empty(T, dtype=float)\n",
    "    nonpref_mean = np.empty(T, dtype=float)\n",
    "\n",
    "    for t in range(T):\n",
    "        d = np.abs(circular_diff_deg((HD_PD_deg + 180.0) % 360.0, real_Az_deg_0_360[t]))\n",
    "        pref_idx = np.where(d <= azimuth_range_deg)[0]\n",
    "        nonpref_idx = np.where(d > azimuth_range_deg)[0]\n",
    "\n",
    "        pref_mean[t] = float(np.mean(rHD[pref_idx, t])) if pref_idx.size else np.nan\n",
    "        nonpref_mean[t] = float(np.mean(rHD[nonpref_idx, t])) if nonpref_idx.size else np.nan\n",
    "\n",
    "    return pref_mean, nonpref_mean\n",
    "\n",
    "\n",
    "def split_by_exclude_list_timeaxis1(rHD_NT: np.ndarray, exclude_list: np.ndarray):\n",
    "    locomotion = np.delete(rHD_NT, exclude_list, axis=1)\n",
    "    stationary = rHD_NT[:, exclude_list]\n",
    "    return locomotion, stationary\n",
    "\n",
    "\n",
    "def split_1d_by_exclude_list(arr_1d: np.ndarray, exclude_list: np.ndarray):\n",
    "    loco = np.delete(arr_1d, exclude_list, axis=0)\n",
    "    stat = arr_1d[exclude_list]\n",
    "    return loco, stat\n",
    "\n",
    "\n",
    "def process_one_run_notebook_exact(mat_path: Path, emb_path: Path, out_dir: Path, azimuth_range_deg=30.0):\n",
    "    run_stem = mat_path.stem\n",
    "    suffix = infer_suffix_from_stem(run_stem)\n",
    "\n",
    "    mat = loadmat(mat_path, squeeze_me=False)  # keep MATLAB-like shapes\n",
    "\n",
    "    rHD = np.asarray(mat[\"rHD\"], dtype=float)         # (N,T)\n",
    "    decodedHD = np.asarray(mat[\"decodedHD\"], dtype=float)  # likely (T,1)\n",
    "    Az = np.asarray(mat[\"Az\"], dtype=float)           # likely (1,T)\n",
    "\n",
    "    # real_Az exactly like notebook\n",
    "    real_Az = (Az + 180) % 360  # (1,T)\n",
    "\n",
    "    T = rHD.shape[1]\n",
    "    if real_Az.shape[1] != T:\n",
    "        raise ValueError(f\"Az length mismatch in {run_stem}: rHD T={T}, Az T={real_Az.shape[1]}\")\n",
    "\n",
    "    # stationary list exactly like notebook\n",
    "    segments_to_exclude_list = build_segments_to_exclude_list_notebook(mat)\n",
    "\n",
    "    # Locomotion & stationary rHD matrices (NOTEBOOK)\n",
    "    locomotion_rHD_matrix, stationary_rHD_matrix = split_by_exclude_list_timeaxis1(rHD, segments_to_exclude_list)\n",
    "\n",
    "    # Angles (NOTEBOOK indexing: real_Az[0] is 1D)\n",
    "    loco_real_Az, stat_real_Az = split_1d_by_exclude_list(real_Az[0], segments_to_exclude_list)\n",
    "\n",
    "    # decodedHD splits (NOTEBOOK uses +180)\n",
    "    loco_decodedHD = np.delete(decodedHD, segments_to_exclude_list, axis=0) + 180\n",
    "    stat_decodedHD = decodedHD[segments_to_exclude_list] + 180\n",
    "\n",
    "    # ---- decoding error EXACT notebook ----\n",
    "    loco_errors = angular_differences_notebook(loco_real_Az, loco_decodedHD)\n",
    "    stat_errors = angular_differences_notebook(stat_real_Az, stat_decodedHD)\n",
    "\n",
    "    loco_errors_list = [loco_errors[i][0] for i in range(len(loco_real_Az))]\n",
    "    stat_errors_list = [stat_errors[i][0] for i in range(len(stat_real_Az))]\n",
    "\n",
    "    mean_loco_errors = float(np.mean(loco_errors_list))\n",
    "    mean_stat_errors = float(np.mean(stat_errors_list)) if len(stat_errors_list) else np.nan\n",
    "\n",
    "    # ---- embedding (already saved) ----\n",
    "    tmp_emb = np.load(emb_path, allow_pickle=True)\n",
    "    tmp_emb = np.asarray(tmp_emb)\n",
    "\n",
    "    # NOTE: your earlier notebook used X = sqrt(rHD.T) for isomap,\n",
    "    # but since you already saved tmp_emb, we just split it.\n",
    "    loco_emb = np.delete(tmp_emb, segments_to_exclude_list, axis=0)\n",
    "    stat_emb = tmp_emb[segments_to_exclude_list, :]\n",
    "\n",
    "    # ---- pref / nonpref mean activity (same as before, but split by same exclude list) ----\n",
    "    pref_mean_t, nonpref_mean_t = compute_pref_nonpref_mean_activity(\n",
    "        rHD=rHD,\n",
    "        real_Az_deg_0_360=real_Az[0],\n",
    "        azimuth_range_deg=azimuth_range_deg,\n",
    "    )\n",
    "\n",
    "    loco_pref_mean_t, stat_pref_mean_t = split_1d_by_exclude_list(pref_mean_t, segments_to_exclude_list)\n",
    "    loco_nonpref_mean_t, stat_nonpref_mean_t = split_1d_by_exclude_list(nonpref_mean_t, segments_to_exclude_list)\n",
    "\n",
    "    mean_loco_pref = float(np.nanmean(loco_pref_mean_t))\n",
    "    mean_loco_nonpref = float(np.nanmean(loco_nonpref_mean_t))\n",
    "    mean_stat_pref = float(np.nanmean(stat_pref_mean_t))\n",
    "    mean_stat_nonpref = float(np.nanmean(stat_nonpref_mean_t))\n",
    "\n",
    "    # ---- betti ----\n",
    "    h0_stat, h1_stat, h2_stat = compute_betti(stat_emb)\n",
    "    h0_loco, h1_loco, h2_loco = compute_betti(loco_emb)\n",
    "\n",
    "    # ---- save ----\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.save(out_dir / f\"mean_loco_errors{suffix}.npy\", mean_loco_errors)\n",
    "    np.save(out_dir / f\"mean_stat_errors{suffix}.npy\", mean_stat_errors)\n",
    "\n",
    "    np.save(out_dir / f\"mean_loco_pref_HD_mean_activities{suffix}.npy\", mean_loco_pref)\n",
    "    np.save(out_dir / f\"mean_loco_non_pref_HD_mean_activities{suffix}.npy\", mean_loco_nonpref)\n",
    "    np.save(out_dir / f\"mean_stat_pref_HD_mean_activities{suffix}.npy\", mean_stat_pref)\n",
    "    np.save(out_dir / f\"mean_stat_non_pref_HD_mean_activities{suffix}.npy\", mean_stat_nonpref)\n",
    "\n",
    "    np.save(out_dir / f\"old_stat_betti_h0{suffix}.npy\", h0_stat)\n",
    "    np.save(out_dir / f\"old_stat_betti_h1{suffix}.npy\", h1_stat)\n",
    "    np.save(out_dir / f\"old_stat_betti_h2{suffix}.npy\", h2_stat)\n",
    "\n",
    "    np.save(out_dir / f\"old_loco_betti_h0{suffix}.npy\", h0_loco)\n",
    "    np.save(out_dir / f\"old_loco_betti_h1{suffix}.npy\", h1_loco)\n",
    "    np.save(out_dir / f\"old_loco_betti_h2{suffix}.npy\", h2_loco)\n",
    "\n",
    "    return {\n",
    "        \"run\": run_stem,\n",
    "        \"suffix\": suffix,\n",
    "        \"mean_loco_errors\": mean_loco_errors,\n",
    "        \"mean_stat_errors\": mean_stat_errors,\n",
    "        \"T\": int(T),\n",
    "        \"n_stat\": int(len(segments_to_exclude_list)),\n",
    "        \"n_loco\": int(T - len(segments_to_exclude_list)),\n",
    "    }\n",
    "\n",
    "\n",
    "def process_directory_notebook_exact(data_dir: str, azimuth_range_deg=30.0):\n",
    "    data_dir = Path(data_dir)\n",
    "    out_dir = data_dir\n",
    "\n",
    "    emb_files = sorted(data_dir.glob(\"*_tmp_emb.npy\"))\n",
    "    if not emb_files:\n",
    "        raise FileNotFoundError(f\"No *_tmp_emb.npy files found in: {data_dir}\")\n",
    "\n",
    "    results = []\n",
    "    for emb_path in emb_files:\n",
    "        stem = emb_path.name.replace(\"_tmp_emb.npy\", \"\")\n",
    "        mat_path = data_dir / f\"{stem}.mat\"\n",
    "        if not mat_path.exists():\n",
    "            print(f\"[SKIP] No matching .mat for: {emb_path.name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[RUN] {stem}\")\n",
    "        res = process_one_run_notebook_exact(mat_path, emb_path, out_dir, azimuth_range_deg=azimuth_range_deg)\n",
    "        results.append(res)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\hz3791\\Documents\\Git_repo_for_Alex_paper\\HD_attractor_model\\data\"\n",
    "\n",
    "results = process_directory_notebook_exact(data_dir, azimuth_range_deg=30.0)\n",
    "print(\"Done:\", len(results))\n",
    "print(results[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a8258",
   "metadata": {},
   "source": [
    "# Plotting figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_sem(x):\n",
    "    x = np.array(x, dtype=float)\n",
    "    m = float(np.mean(x))\n",
    "    sem = float(np.std(x, ddof=1) / np.sqrt(len(x))) if len(x) > 1 else 0.0\n",
    "    return m, sem\n",
    "\n",
    "\n",
    "def max_h1_length(betti):\n",
    "    if len(betti) == 0:\n",
    "        return 0.0\n",
    "    betti = np.asarray(betti)\n",
    "    betti = betti[np.isfinite(betti).all(axis=1)] if betti.size else betti\n",
    "    if betti.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.max(betti[:, 1] - betti[:, 0]))\n",
    "\n",
    "\n",
    "def load_results_only(data_dir):\n",
    "    results = []\n",
    "\n",
    "    # Find all mean_loco_errors*.npy\n",
    "    files = sorted(glob.glob(os.path.join(data_dir, \"mean_loco_errors*.npy\")))\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(\"No mean_loco_errors*.npy files found – run pipeline first.\")\n",
    "\n",
    "    for f in files:\n",
    "        base = os.path.basename(f)                 # e.g. \"mean_loco_errors_2.npy\"\n",
    "        suffix = base[len(\"mean_loco_errors\"):-4]  # → \"_2\"  or \"\"\n",
    "\n",
    "        def load(name):\n",
    "            p = os.path.join(data_dir, f\"{name}{suffix}.npy\")\n",
    "            return np.load(p, allow_pickle=True)\n",
    "\n",
    "        r = {\n",
    "            \"mean_loco_errors\": float(load(\"mean_loco_errors\")),\n",
    "            \"mean_stat_errors\": float(load(\"mean_stat_errors\")),\n",
    "\n",
    "            \"mean_loco_pref\": float(load(\"mean_loco_pref_HD_mean_activities\")),\n",
    "            \"mean_loco_nonpref\": float(load(\"mean_loco_non_pref_HD_mean_activities\")),\n",
    "\n",
    "            \"mean_stat_pref\": float(load(\"mean_stat_pref_HD_mean_activities\")),\n",
    "            \"mean_stat_nonpref\": float(load(\"mean_stat_non_pref_HD_mean_activities\")),\n",
    "\n",
    "            \"h1_len_loco\": max_h1_length(load(\"old_loco_betti_h1\")),\n",
    "            \"h1_len_stat\": max_h1_length(load(\"old_stat_betti_h1\")),\n",
    "        }\n",
    "\n",
    "        results.append(r)\n",
    "\n",
    "    print(f\"Loaded {len(results)} simulations from cached results.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def stationary_index_list_from_ranges(starts, ends, T: int) -> np.ndarray:\n",
    "    starts = np.asarray(starts).reshape(-1)\n",
    "    ends   = np.asarray(ends).reshape(-1)\n",
    "\n",
    "    # autodetect 0-based vs 1-based (same as your pipeline)\n",
    "    if np.any(starts == 0) or np.any(ends == 0):\n",
    "        starts0 = starts.astype(int)\n",
    "        ends0   = ends.astype(int)\n",
    "    else:\n",
    "        starts0 = (starts.astype(int) - 1)\n",
    "        ends0   = (ends.astype(int) - 1)\n",
    "\n",
    "    idx = []\n",
    "    for s, e in zip(starts0, ends0):\n",
    "        s = max(0, min(T - 1, s))\n",
    "        e = max(0, min(T - 1, e))\n",
    "        if e >= s:\n",
    "            idx.append(np.arange(s, e + 1, dtype=int))\n",
    "    if not idx:\n",
    "        return np.array([], dtype=int)\n",
    "    return np.unique(np.concatenate(idx))\n",
    "\n",
    "\n",
    "def load_representation(data_dir, rep_base):\n",
    "    mat_path = os.path.join(data_dir, rep_base + \".mat\")\n",
    "    if not os.path.exists(mat_path):\n",
    "        raise FileNotFoundError(f\"Missing: {mat_path}\")\n",
    "\n",
    "    m = sio.loadmat(mat_path, squeeze_me=True)\n",
    "\n",
    "    rHD = np.asarray(m[\"rHD\"], dtype=float)              # (N, T)\n",
    "    Az = np.asarray(m[\"Az\"], dtype=float).reshape(-1)    # (T,)\n",
    "    decodedHD = np.asarray(m.get(\"decodedHD\", []), dtype=float).reshape(-1) if \"decodedHD\" in m else None\n",
    "\n",
    "    T = rHD.shape[1]\n",
    "    starts = m[\"stat_startIndices\"]\n",
    "    ends   = m[\"stat_endIndices\"]\n",
    "    segments_to_exclude_list = stationary_index_list_from_ranges(starts, ends, T)  # 0-based\n",
    "\n",
    "    emb_path = os.path.join(data_dir, rep_base + \"_tmp_emb.npy\")\n",
    "    if not os.path.exists(emb_path):\n",
    "        raise FileNotFoundError(f\"Missing embedding: {emb_path}\")\n",
    "    tmp_emb = np.load(emb_path, allow_pickle=True)\n",
    "    tmp_emb = np.asarray(tmp_emb)\n",
    "    if tmp_emb.ndim != 2 or tmp_emb.shape[1] < 2:\n",
    "        raise ValueError(f\"Embedding has shape {tmp_emb.shape}, expected (N,>=2)\")\n",
    "\n",
    "    # suffix \"\" or \"_k\" inferred from rep_base tail\n",
    "    suffix = \"\"\n",
    "    tail = rep_base.split(\"_\")[-1]\n",
    "    if tail.isdigit():\n",
    "        suffix = \"_\" + tail\n",
    "\n",
    "    def load_npy(name):\n",
    "        return np.load(os.path.join(data_dir, f\"{name}{suffix}.npy\"), allow_pickle=True)\n",
    "\n",
    "    rep = {\n",
    "        \"rep_base\": rep_base,\n",
    "        \"suffix\": suffix,\n",
    "        \"rHD\": rHD,\n",
    "        \"Az\": Az,\n",
    "        \"decodedHD\": decodedHD,\n",
    "        \"segments_to_exclude_list\": segments_to_exclude_list,\n",
    "        \"tmp_emb\": tmp_emb,  # full embedding (run+stat)\n",
    "        \"h0_loco\": load_npy(\"old_loco_betti_h0\"),\n",
    "        \"h1_loco\": load_npy(\"old_loco_betti_h1\"),\n",
    "        \"h2_loco\": load_npy(\"old_loco_betti_h2\"),\n",
    "        \"h0_stat\": load_npy(\"old_stat_betti_h0\"),\n",
    "        \"h1_stat\": load_npy(\"old_stat_betti_h1\"),\n",
    "        \"h2_stat\": load_npy(\"old_stat_betti_h2\"),\n",
    "    }\n",
    "    return rep\n",
    "\n",
    "\n",
    "def get_long_bettibars(h0, h1, h2):\n",
    "    out = []\n",
    "    lengths = []\n",
    "\n",
    "    for H in [h0, h1, h2]:\n",
    "        H = np.asarray(H)\n",
    "\n",
    "        # Remove rows with NaN or Inf\n",
    "        if H.size > 0:\n",
    "            finite_mask = np.isfinite(H).all(axis=1)\n",
    "            H = H[finite_mask]\n",
    "\n",
    "        if H.size == 0:\n",
    "            out.append([])\n",
    "            lengths.append([])\n",
    "            continue\n",
    "\n",
    "        lens = (H[:, 1] - H[:, 0]).astype(float)\n",
    "\n",
    "        out.append(H)\n",
    "        lengths.append(lens)\n",
    "\n",
    "    return out, lengths\n",
    "\n",
    "\n",
    "def compute_corr_strip_like_notebook(rHD_loco_TN, rHD_stat_TN, seed=0):\n",
    "    T_loco = rHD_loco_TN.shape[0]\n",
    "    T_stat = rHD_stat_TN.shape[0]\n",
    "    n = min(T_loco, T_stat)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # subsample timepoints (random, without replacement)\n",
    "    loco_idx = rng.choice(T_loco, size=n, replace=False)\n",
    "    stat_idx = rng.choice(T_stat, size=n, replace=False)\n",
    "\n",
    "    loco_sub = rHD_loco_TN[loco_idx]\n",
    "    stat_sub = rHD_stat_TN[stat_idx]\n",
    "\n",
    "    loco_rHD_corr = np.corrcoef(loco_sub)  # (n, n)\n",
    "    stat_rHD_corr = np.corrcoef(stat_sub)  # (n, n)\n",
    "\n",
    "    upper = np.triu_indices(n, k=1)\n",
    "    loco_vals = loco_rHD_corr[upper]\n",
    "    stat_vals = stat_rHD_corr[upper]\n",
    "\n",
    "    sorted_idx = np.argsort(loco_vals)\n",
    "    sorted_loco = loco_vals[sorted_idx]\n",
    "    sorted_stat = stat_vals[sorted_idx]\n",
    "    diff = sorted_loco - sorted_stat\n",
    "\n",
    "    corr_for_plot_list = np.array([sorted_loco, sorted_stat, diff]).T\n",
    "    return corr_for_plot_list\n",
    "\n",
    "\n",
    "def plot_panels_B_to_G(results_all, rep, dt=0.1, save_path=None, title=None):\n",
    "\n",
    "    # ---- colors for bars ----\n",
    "    stat_color = \"#8fd2d9\"\n",
    "    run_color  = \"#3b9099\"\n",
    "\n",
    "    # ---- Aggregate scalars across simulations (for error bars) ----\n",
    "    loco_errs = [r[\"mean_loco_errors\"] for r in results_all]\n",
    "    stat_errs = [r[\"mean_stat_errors\"] for r in results_all]\n",
    "\n",
    "    loco_pref = [r[\"mean_loco_pref\"] for r in results_all]\n",
    "    loco_nonpref = [r[\"mean_loco_nonpref\"] for r in results_all]\n",
    "    stat_pref = [r[\"mean_stat_pref\"] for r in results_all]\n",
    "    stat_nonpref = [r[\"mean_stat_nonpref\"] for r in results_all]\n",
    "\n",
    "    h1_loco = [r[\"h1_len_loco\"] for r in results_all]\n",
    "    h1_stat = [r[\"h1_len_stat\"] for r in results_all]\n",
    "\n",
    "    loco_err_m, loco_err_sem = mean_and_sem(loco_errs)\n",
    "    stat_err_m, stat_err_sem = mean_and_sem(stat_errs)\n",
    "\n",
    "    loco_pref_m, loco_pref_sem = mean_and_sem(loco_pref)\n",
    "    loco_nonpref_m, loco_nonpref_sem = mean_and_sem(loco_nonpref)\n",
    "    stat_pref_m, stat_pref_sem = mean_and_sem(stat_pref)\n",
    "    stat_nonpref_m, stat_nonpref_sem = mean_and_sem(stat_nonpref)\n",
    "\n",
    "    h1_loco_m, h1_loco_sem = mean_and_sem(h1_loco)\n",
    "    h1_stat_m, h1_stat_sem = mean_and_sem(h1_stat)\n",
    "\n",
    "    # ---- Prep representative split (MATCH YOUR PIPELINE) ----\n",
    "    rHD = rep[\"rHD\"]  # (N, T)\n",
    "    Az = rep[\"Az\"].reshape(-1)  # (T,)\n",
    "    real_Az = (Az + 180.0) % 360.0  # notebook\n",
    "\n",
    "    stat_idx = rep[\"segments_to_exclude_list\"].astype(int)\n",
    "    T = rHD.shape[1]\n",
    "\n",
    "    # build loco indices as complement (like np.delete in notebook)\n",
    "    mask = np.ones(T, dtype=bool)\n",
    "    mask[stat_idx] = False\n",
    "    loco_idx = np.where(mask)[0]\n",
    "\n",
    "    # embeddings split to match pipeline: tmp_emb has time on axis 0\n",
    "    tmp_emb = rep[\"tmp_emb\"]\n",
    "    loco_emb = tmp_emb[loco_idx]\n",
    "    stat_emb = tmp_emb[stat_idx]\n",
    "\n",
    "    loco_real_Az = real_Az[loco_idx]\n",
    "    stat_real_Az = real_Az[stat_idx]\n",
    "\n",
    "    # Also rHD for corr strip wants time x neuron (T,N)\n",
    "    loco_rHD_TN = rHD[:, loco_idx].T\n",
    "    stat_rHD_TN = rHD[:, stat_idx].T\n",
    "\n",
    "    # ---- Figure layout: mimic your notebook D structure inside panel area ----\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    outer = gridspec.GridSpec(\n",
    "        2, 3,\n",
    "        width_ratios=[1.6, 1.0, 1.0],\n",
    "        height_ratios=[1.0, 1.0],\n",
    "        wspace=0.35, hspace=0.45\n",
    "    )\n",
    "\n",
    "    # ----------------\n",
    "    # Panel B: heatmap\n",
    "    # ----------------\n",
    "    axB = fig.add_subplot(outer[0, 0])\n",
    "    imB = axB.imshow(rHD, aspect=\"auto\", origin=\"lower\")\n",
    "    axB.set_title(\"B: rHD heatmap (rep sim)\")\n",
    "    axB.set_xlabel(\"Time (samples)\")\n",
    "    axB.set_ylabel(\"Neuron ID\")\n",
    "    if len(stat_idx) > 0:\n",
    "        axB.axvline(int(stat_idx[0]), linestyle=\"--\", linewidth=1)\n",
    "    cbB = fig.colorbar(imB, ax=axB, fraction=0.046, pad=0.04)\n",
    "    cbB.set_label(\"rHD\")\n",
    "    axB.set_xlim([260, 360])\n",
    "\n",
    "    # --------------------------\n",
    "    # Panel C: Kendall tau\n",
    "    # --------------------------\n",
    "    axC = fig.add_subplot(outer[0, 1])\n",
    "\n",
    "    corr_for_plot_list = compute_corr_strip_like_notebook(loco_rHD_TN, stat_rHD_TN, seed=0)\n",
    "    print(\"T_loco:\", loco_rHD_TN.shape[0], \"T_stat:\", stat_rHD_TN.shape[0])\n",
    "\n",
    "    colors = [(0, '#3853A4'), (0.5, 'white'), (1, '#ED1F24')]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_colormap', colors)\n",
    "\n",
    "    imC = axC.imshow(\n",
    "        corr_for_plot_list[:, :3],\n",
    "        cmap=custom_cmap,\n",
    "        aspect='auto',\n",
    "        vmin=-1, vmax=1,\n",
    "        interpolation='none',\n",
    "        origin='lower'\n",
    "    )\n",
    "    axC.set_title(\"C: Kendall tau\")\n",
    "    axC.set_xticks([0, 1, 2])\n",
    "    axC.set_xticklabels([\"Run\", \"Stat\", \"Diff\"])\n",
    "    axC.set_yticks([])\n",
    "\n",
    "    axC.spines['right'].set_visible(False)\n",
    "    axC.spines['top'].set_visible(False)\n",
    "    plt.setp(axC.spines.values(), linewidth=1.5)\n",
    "    axC.xaxis.set_tick_params(width=1.5)\n",
    "    axC.yaxis.set_tick_params(width=1.5)\n",
    "\n",
    "    # --------------------------\n",
    "    # Panel D: embeddings + barcodes\n",
    "    # --------------------------\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(\n",
    "        6, 6, subplot_spec=outer[0, 2], wspace=0.0, hspace=0.25\n",
    "    )\n",
    "\n",
    "    # Top row: loco scatter (cols 0:3) and stat scatter (cols 3:6)\n",
    "    axD_loco = fig.add_subplot(inner[0:3, 0:3])\n",
    "    axD_stat = fig.add_subplot(inner[0:3, 3:6])\n",
    "\n",
    "    axD_loco.scatter(loco_emb[:, 0], loco_emb[:, 1], s=1,\n",
    "                     c=loco_real_Az, vmin=0, vmax=360, cmap='hsv')\n",
    "    axD_loco.axis('off')\n",
    "\n",
    "    axD_stat.scatter(stat_emb[:, 0], stat_emb[:, 1], s=1,\n",
    "                     c=stat_real_Az, vmin=0, vmax=360, cmap='hsv')\n",
    "    axD_stat.axis('off')\n",
    "\n",
    "    # Barcodes underneath: 3 rows (H0,H1,H2) for loco and stat\n",
    "    col_list = ['red', 'green', 'purple']\n",
    "\n",
    "    to_plot_loco, _ = get_long_bettibars(rep[\"h0_loco\"], rep[\"h1_loco\"], rep[\"h2_loco\"])\n",
    "    to_plot_stat, _ = get_long_bettibars(rep[\"h0_stat\"], rep[\"h1_stat\"], rep[\"h2_stat\"])\n",
    "\n",
    "    # compute xmax = largest finite value present in any bar interval (both columns, loco+stat, all bettis)\n",
    "    def _max_finite_endpoint(barsets):\n",
    "        mx = 0.0\n",
    "        for bars in barsets:\n",
    "            B = np.asarray(bars)\n",
    "            if B.size == 0:\n",
    "                continue\n",
    "            B = B[np.isfinite(B).all(axis=1)]\n",
    "            if B.size == 0:\n",
    "                continue\n",
    "            mx = max(mx, float(np.max(B)))  \n",
    "        return mx\n",
    "\n",
    "    xmax = max(_max_finite_endpoint(to_plot_loco), _max_finite_endpoint(to_plot_stat))\n",
    "    if not np.isfinite(xmax) or xmax <= 0:\n",
    "        xmax = 1.0\n",
    "\n",
    "    axL_base = None\n",
    "    axS_base = None\n",
    "\n",
    "    for curr_betti in range(3):\n",
    "        axL = fig.add_subplot(\n",
    "            inner[curr_betti + 3: curr_betti + 4, 0:3],\n",
    "            sharex=axL_base if axL_base is not None else None\n",
    "        )\n",
    "        if axL_base is None:\n",
    "            axL_base = axL\n",
    "\n",
    "        bars = np.asarray(to_plot_loco[curr_betti])\n",
    "        if bars.size != 0:\n",
    "            bars = bars[np.isfinite(bars).all(axis=1)]\n",
    "            for i, interval in enumerate(bars):\n",
    "                axL.plot([interval[0], interval[1]], [i, i],\n",
    "                         color=col_list[curr_betti], lw=3)\n",
    "\n",
    "        axL.set_xlim(0, xmax)\n",
    "        axL.set_yticks([])\n",
    "        axL.spines['right'].set_visible(False)\n",
    "        axL.spines['top'].set_visible(False)\n",
    "        plt.setp(axL.spines.values(), linewidth=1.5)\n",
    "        axL.xaxis.set_tick_params(width=1.5)\n",
    "        axL.yaxis.set_tick_params(width=1.5)\n",
    "\n",
    "        if curr_betti != 2:\n",
    "            axL.set_xticks([])\n",
    "            axL.tick_params(labelbottom=False)\n",
    "        else:\n",
    "            axL.set_xticks([])  # keep notebook style (no ticks)\n",
    "\n",
    "        # stat barcode axis (sharex)\n",
    "        axS = fig.add_subplot(\n",
    "            inner[curr_betti + 3: curr_betti + 4, 3:6],\n",
    "            sharex=axS_base if axS_base is not None else None\n",
    "        )\n",
    "        if axS_base is None:\n",
    "            axS_base = axS\n",
    "\n",
    "        bars = np.asarray(to_plot_stat[curr_betti])\n",
    "        if bars.size != 0:\n",
    "            bars = bars[np.isfinite(bars).all(axis=1)]\n",
    "            for i, interval in enumerate(bars):\n",
    "                axS.plot([interval[0], interval[1]], [i, i],\n",
    "                         color=col_list[curr_betti], lw=1.5)\n",
    "\n",
    "        axS.set_xlim(0, xmax)\n",
    "        axS.set_yticks([])\n",
    "        axS.spines['right'].set_visible(False)\n",
    "        axS.spines['top'].set_visible(False)\n",
    "        plt.setp(axS.spines.values(), linewidth=1.5)\n",
    "        axS.xaxis.set_tick_params(width=1.5)\n",
    "        axS.yaxis.set_tick_params(width=1.5)\n",
    "\n",
    "        if curr_betti != 2:\n",
    "            axS.set_xticks([])\n",
    "            axS.tick_params(labelbottom=False)\n",
    "        else:\n",
    "            axS.set_xticks([])  \n",
    "\n",
    "    # --------------------------\n",
    "    # Panel E: mean firing rates (across sims)\n",
    "    # --------------------------\n",
    "    axE = fig.add_subplot(outer[1, 0])\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "\n",
    "    axE.bar(x - width/2, [stat_pref_m, stat_nonpref_m], width,\n",
    "            yerr=[stat_pref_sem, stat_nonpref_sem], capsize=4, label=\"Stat\",\n",
    "            color=stat_color)\n",
    "    axE.bar(x + width/2, [loco_pref_m, loco_nonpref_m], width,\n",
    "            yerr=[loco_pref_sem, loco_nonpref_sem], capsize=4, label=\"Run\",\n",
    "            color=run_color)\n",
    "    axE.set_xticks(x)\n",
    "    axE.set_xticklabels([\"Az pref.\", \"Az non-pr.\"])\n",
    "    axE.set_ylabel(\"Mean firing (a.u.)\")\n",
    "    axE.set_title(\"E: Mean firing rate (across sims)\")\n",
    "    axE.legend()\n",
    "\n",
    "    # --------------------------\n",
    "    # Panel F: decoding error (across sims)\n",
    "    # --------------------------\n",
    "    axF = fig.add_subplot(outer[1, 1])\n",
    "    axF.bar([0, 1], [stat_err_m, loco_err_m],\n",
    "            yerr=[stat_err_sem, loco_err_sem], capsize=4,\n",
    "            color=[stat_color, run_color])\n",
    "    axF.set_xticks([0, 1])\n",
    "    axF.set_xticklabels([\"Stat\", \"Run\"])\n",
    "    axF.set_ylabel(\"Decoding err. (deg)\")\n",
    "    axF.set_title(\"F: Decoding error (across sims)\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Panel G: H1 length (across sims)\n",
    "    # --------------------------\n",
    "    axG = fig.add_subplot(outer[1, 2])\n",
    "    axG.bar([0, 1], [h1_stat_m, h1_loco_m],\n",
    "            yerr=[h1_stat_sem, h1_loco_sem], capsize=4,\n",
    "            color=[stat_color, run_color])\n",
    "    axG.set_xticks([0, 1])\n",
    "    axG.set_xticklabels([\"Stat\", \"Run\"])\n",
    "    axG.set_ylabel(\"H1 length\")\n",
    "    axG.set_title(\"G: H1 length (across sims)\")\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, y=1.02)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63821a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\home\\...\\HD_attractor_model\\data\"\n",
    "\n",
    "results = load_results_only(data_dir)\n",
    "\n",
    "rep_base = \"Inh_gain6Exc_gain9bimodal_vis_inp0gaus_ratio1visual_std0.262_rand_shuffle_1\"\n",
    "rep = load_representation(data_dir, rep_base)\n",
    "\n",
    "plot_panels_B_to_G(results_all=results, rep=rep, dt=0.1, save_path=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
